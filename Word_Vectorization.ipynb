{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Vectorization",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMKNAnS+U0nyhkputCEWGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/notnikhilreddy/Word-Vectorization/blob/master/Word_Vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgF49RnJBs0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygO3oCApDhVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles1.xml-p1p30303.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNBJrdz1jxHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xlDdOJUCrys",
        "colab_type": "code",
        "outputId": "115f25d1-5238-45cb-81d7-46105c543efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# CONVERT RAW WIKIPEDIA DUMP FILE INTO CORPUS\n",
        "\n",
        "import sys\n",
        "from gensim.corpora import WikiCorpus\n",
        "from IPython.display import HTML, display\n",
        "from datetime import datetime\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "\n",
        "in_f = \"word2vec/enwiki-latest-pages-articles1.xml-p1p30303.bz2\"\n",
        "out_f = \"word2vec/wiki_en2.txt\"\n",
        "print(\"Started at :\", datetime.now())\n",
        "articles_count = display(progress(0, 6074991), display_id=True)\n",
        "# make_corpus(in_f, out_f)\n",
        "\n",
        "\"\"\"Convert Wikipedia xml dump file to text corpus\"\"\"\n",
        "wiki = WikiCorpus(in_f)\n",
        "\n",
        "print(\"Started For loop\")\n",
        "i = 0\n",
        "for text in wiki.get_texts():\n",
        "\toutput = open(out_f, 'a')\n",
        "\toutput.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
        "\tarticles_count.update(progress(i, 100))\n",
        "\ti = i + 1\n",
        "\tif (i % 10000 == 0):\n",
        "\t\tprint(\"Articles Finished :\", i)\n",
        "\toutput.close()\n",
        "\n",
        "print('Processing complete!')\n",
        "print(\"Finished at :\", datetime.now())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started at : 2020-05-15 04:09:28.493649\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='14873'\n",
              "            max='100',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            14873\n",
              "        </progress>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Started For loop\n",
            "Articles Finished : 10000\n",
            "Processing complete!\n",
            "Finished at : 2020-05-15 04:16:23.389175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0UH_VrkR1xD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "31c9d722-e28b-47cc-cf46-bcd68fe40182"
      },
      "source": [
        "# GENERATE UNIQUE WORDS SET FROM CORPUS\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "all_words_count = 0;\n",
        "unique_words = set()\n",
        "\n",
        "# all_words_file = open(out_f, 'w')\n",
        "input_file = \"/content/drive/My Drive/Projects/word2vec/data/wiki_en_15000.txt\"\n",
        "\n",
        "line_no = 0\n",
        "with open (input_file, \"r\") as corpus:\n",
        "  for line in corpus:\n",
        "    line_no += 1\n",
        "    tokenize_word = word_tokenize(line.strip())\n",
        "    for word in tokenize_word:\n",
        "      unique_words.add(word)\n",
        "      all_words_count += 1\n",
        "    if(line_no % 500 == 0):\n",
        "      print(\"Articles: \" + str(line_no) + \"\\t\" + \"All words: \" + str(all_words_count) + \"\\t\" + \"Unique words: \" + str(len(unique_words)))\n",
        "      # all_words_file.write(bytes(word, 'utf-8').decode('utf-8') + '\\n')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Articles: 500\tAll words: 1842184\tUnique words: 84040\n",
            "Articles: 1000\tAll words: 3352260\tUnique words: 121275\n",
            "Articles: 1500\tAll words: 4706629\tUnique words: 148760\n",
            "Articles: 2000\tAll words: 6421775\tUnique words: 173107\n",
            "Articles: 2500\tAll words: 8178528\tUnique words: 196001\n",
            "Articles: 3000\tAll words: 9851328\tUnique words: 216955\n",
            "Articles: 3500\tAll words: 11400816\tUnique words: 235381\n",
            "Articles: 4000\tAll words: 13059281\tUnique words: 254668\n",
            "Articles: 4500\tAll words: 14764654\tUnique words: 272056\n",
            "Articles: 5000\tAll words: 16220838\tUnique words: 290112\n",
            "Articles: 5500\tAll words: 17886506\tUnique words: 308630\n",
            "Articles: 6000\tAll words: 19539454\tUnique words: 327016\n",
            "Articles: 6500\tAll words: 21411232\tUnique words: 343665\n",
            "Articles: 7000\tAll words: 23345023\tUnique words: 361409\n",
            "Articles: 7500\tAll words: 25079423\tUnique words: 377340\n",
            "Articles: 8000\tAll words: 26804319\tUnique words: 392966\n",
            "Articles: 8500\tAll words: 28281702\tUnique words: 409258\n",
            "Articles: 9000\tAll words: 29872293\tUnique words: 428669\n",
            "Articles: 9500\tAll words: 31647924\tUnique words: 444248\n",
            "Articles: 10000\tAll words: 33296924\tUnique words: 458389\n",
            "Articles: 10500\tAll words: 34942466\tUnique words: 470772\n",
            "Articles: 11000\tAll words: 36631832\tUnique words: 485812\n",
            "Articles: 11500\tAll words: 38335605\tUnique words: 497963\n",
            "Articles: 12000\tAll words: 39675248\tUnique words: 508120\n",
            "Articles: 12500\tAll words: 41174806\tUnique words: 517778\n",
            "Articles: 13000\tAll words: 42944297\tUnique words: 529049\n",
            "Articles: 13500\tAll words: 44673065\tUnique words: 540621\n",
            "Articles: 14000\tAll words: 46331992\tUnique words: 552393\n",
            "Articles: 14500\tAll words: 47928759\tUnique words: 561907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP22YQoZv4xT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WRITE UNIQUE WORDS TO A FILE (not really useful)\n",
        "\n",
        "unique_words = sorted(unique_words)\n",
        "unique_words_file = \"/content/drive/My Drive/Projects/word2vec/data/unique_words_file.txt\"\n",
        "with open(unique_words_file, \"w\") as f:\n",
        "  for word in unique_words:\n",
        "    f.write(bytes(word, 'utf-8').decode('utf-8') + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjRPUtlBR4au",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "98458c52-ff52-45c4-871c-a4f35a0a8f6d"
      },
      "source": [
        "print(\"No. of words:\\t\" + str(all_words_count))\n",
        "# unique_words = set(all_words)\n",
        "print(\"No. of unique words:\\t\" + str(len(unique_words)))\n",
        "\n",
        "# print(unique_words[:100])\n",
        "# lines = 0\n",
        "# with open(unique_words_file, \"r\") as uwords:\n",
        "#   for line in uwords:\n",
        "#     lines += 1\n",
        "# print(lines)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of words:\t49300120\n",
            "No. of unique words:\t571050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDbEwr0ZSHMo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "730aae6e-2f35-411b-c4df-338cb2d504d9"
      },
      "source": [
        "# FIT A TOKENIZER OR UNIQUE WORDS\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(lower=True, split=' ', oov_token=\"0\")\n",
        "tokenizer.fit_on_texts([list(unique_words)])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b3a4398aa15f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'unique_words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8b2BZhBJVPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CREATE A TOKENIZED DATABASE\n",
        "\n",
        "output = open(\"/content/drive/My Drive/Projects/word2vec/data/wiki_en_15000_tokenized.txt\", \"w\")\n",
        "\n",
        "with open(\"/content/drive/My Drive/Projects/word2vec/data/wiki_en_15000.txt\", \"r\") as corpus:\n",
        "  for line in corpus:\n",
        "    # print(line)\n",
        "    sequence = tokenizer.texts_to_sequences([line])\n",
        "    # print+(sequence)\n",
        "    # break\n",
        "    for i in sequence[0]:\n",
        "      output.write(bytes(str(i), 'utf-8').decode('utf-8') + ' ')\n",
        "    output.write('\\n')\n",
        "\n",
        "output.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fZEBh18hInZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24957649-4d90-4ebc-c475-34375e43f456"
      },
      "source": [
        "print(len(tokenizer.word_index))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "571051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdRFjSOAyAaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_words_count = 571051"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIAHS7ys3ceF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "61e24a74-a3c4-4301-a2db-5eefcf037730"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "\n",
        "def to_one_hot(data_point_index, vocab_size):\n",
        "  temp = np.zeros(vocab_size)\n",
        "  if data_point_index >= 0:\n",
        "    temp[data_point_index] = 1\n",
        "  return temp\n",
        "\n",
        "csvfile = open('/content/drive/My Drive/Projects/word2vec/data/wiki_en_15000_onehot.csv', 'w', newline='')\n",
        "csvwriter = csv.writer(csvfile, delimiter=',', lineterminator='\\n')\n",
        "# csvfile_y = open('/content/drive/My Drive/Projects/word2vec/data/wiki_en_15000_y_train.csv', 'w', newline='')\n",
        "# csvwriter_y = csv.writer(csvfile_y, delimiter=',', lineterminator='\\n')\n",
        "\n",
        "with open(\"/content/drive/My Drive/Projects/word2vec/data/wiki_en_15000_tokenized.txt\", 'r') as data:\n",
        "  for line in data:\n",
        "    # print(line.strip().split(' '))\n",
        "    # break\n",
        "    words = [int(i) for i in line.strip().split(' ')]\n",
        "    words_count = len(words)\n",
        "\n",
        "    for word in words:\n",
        "      onehot = to_one_hot(word, unique_words_count)\n",
        "      csvwriter.writerow(onehot)\n",
        "\n",
        "csvfile.close()\n",
        "# csvfile_y.close()\n",
        "    # unique_words_count = len()\n",
        "\n",
        "    # x_train = []\n",
        "    # y_train = []\n",
        "\n",
        "    # for window in range(words_count):\n",
        "    #   one = to_one_hot(words[window], unique_words_count)\n",
        "    #   two = to_one_hot(words[window+1], unique_words_count)\n",
        "    #   three = to_one_hot(-1, unique_words_count)\n",
        "    #   four = to_one_hot(words[window+3], unique_words_count)\n",
        "    #   five = to_one_hot(words[window+4], unique_words_count)\n",
        "\n",
        "    #   # group = one.add(two).append(three).append(four).append(five)\n",
        "    #   x_train = np.concatenate((one, two, three, four, five), axis=0)\n",
        "    #   y_train = to_one_hot(words[window+2], unique_words_count)\n",
        "\n",
        "    #   csvwriter_x.writerow(x_train)\n",
        "    #   csvwriter_y.writerow(y_train)\n",
        "      # x_train.append(group)\n",
        "      # y_train.append(to_one_hot(words[window+2], unique_words_count))\n",
        "      # break\n",
        "\n",
        "    # x_train = np.asarray(x_train)\n",
        "    # y_train = np.asarray(y_train)\n",
        "    # break\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-56caa71e867b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0monehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_words_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0mcsvwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}